üöÄ Plan de Migraci√≥n: Embeddings a OpenAI

1. Resumen del Objetivo

- Problema: El backend de FastAPI (con LangChain) est√° demasiado pesado para el plan gratuito de Render debido a dependencias de embeddings locales (p. ej., `sentence-transformers`, que arrastra paquetes grandes como `torch` y `transformers`). Esto impacta el tama√±o de la build, el consumo de RAM al inicio y la velocidad de deploy.
- Objetivo: Migrar la l√≥gica de generaci√≥n de embeddings del modelo local a la API de OpenAI usando `langchain-openai` con el modelo `text-embedding-3-small`. Mantener exactamente la funcionalidad actual del RAG (ingesta, b√∫squeda por similitud y generaci√≥n de respuestas), reduciendo la huella de dependencias y el consumo de recursos.

2. An√°lisis del Flujo Actual

Archivos donde se inicializa/usa el modelo de embeddings:
- `backend/rag/embeddings/embedding_manager.py`
  - Inicializa embeddings con dos modos:
    - Local: carga perezosa de `SentenceTransformer` (modelo por defecto `all-MiniLM-L6-v2`).
    - OpenAI: si el nombre del modelo empieza por `openai:`, usa `OpenAIEmbeddings` de `langchain-openai`.
  - Funciones clave:
    - `embed_documents(texts: List[str]) -> List[List[float]]`: genera embeddings para lotes de textos.
    - `embed_query(query: str) -> List[float]`: genera embedding para consultas.
    - `async embed_text(text: str) -> List[float]`: helper async usado para pings y verificaciones, delega en `embed_query`.
    - `get_embedding_model()`: devuelve el objeto del proveedor (OpenAI o `SentenceTransformer`).

- `backend/api/app.py`
  - Instancia `EmbeddingManager(model_name=settings.embedding_model)` en el arranque y lo inyecta en `VectorStore`, `RAGIngestor` y `RAGRetriever`.
  - Hace un ping ligero de embeddings con `await app.state.embedding_manager.embed_text("ping")`.
  - Ajusta niveles de log para `sentence_transformers` y `transformers` (ruido de librer√≠as locales).

- `backend/rag/ingestion/ingestor.py`
  - Usa `self.embedding_manager.embed_documents(chunk_texts)` en `_deduplicate_chunks` para eliminar duplicados por similitud antes de indexar.
  - Agrega documentos y, si est√°n disponibles, embeddings precomputados al `VectorStore` por lotes.
  - Funciones clave que dependen de embeddings:
    - `ingest_single_pdf(...)`
    - `_deduplicate_chunks(chunks, return_embeddings=True)` (deduplicaci√≥n con embeddings).
    - `_add_batch_to_vector_store(...)` (agregado con embeddings opcionales).

- `backend/rag/vector_store/vector_store.py`
  - Inicializa `Chroma` como almacenamiento vectorial con `embedding_function=self.embedding_function` (inyectado, puede ser `EmbeddingManager`).
  - Calcula embeddings de documentos puntualmente si no est√°n en metadatos:
    - `_get_document_embedding(content: str) -> np.ndarray` usando `embed_query` o `encode` seg√∫n disponibilidad.
  - B√∫squeda por similitud en Chroma v√≠a `similarity_search_by_vector_with_relevance_scores` usando el embedding de la consulta.
  - Funciones clave:
    - `_initialize_store()` (configuraci√≥n y documento dummy opcional).
    - `_get_document_embedding(...)`.
    - `_similarity_search(query_embedding, k, filter)`.
    - `_mmr_search(...)` (aplica MMR sobre resultados iniciales).

- `backend/rag/retrieval/retriever.py`
  - Usa `self.embedding_manager.embed_query(...)` en:
    - `_semantic_reranking(query, docs)`: reordena candidatos por similitud (coseno) + se√±ales.
    - `_apply_mmr(query, docs, k, lambda_mult)`: diversidad de resultados mediante MMR.
  - Importa `HuggingFaceEmbeddings` pero no instancia (resto de c√≥digo se apoya en `EmbeddingManager`).

- `backend/config.py`
  - Configuraci√≥n del modelo de embeddings: `embedding_model` por defecto `sentence-transformers/all-MiniLM-L6-v2` y `default_embedding_dimension`=384 (usado en fallbacks).

3. Plan de Acci√≥n (Formato To-Do List)

[ ] Tarea 1: Refactorizar L√≥gica de Embeddings

- Archivo(s) a modificar: 
  - `backend/rag/embeddings/embedding_manager.py`
  - `backend/config.py`
  - `backend/api/app.py` (solo ajustes de configuraci√≥n/logs si aplica)

- Acci√≥n: Reemplazar la instanciaci√≥n de `SentenceTransformer`/modo local por `OpenAIEmbeddings` de `langchain-openai` como modo por defecto.
  - Usar `OpenAIEmbeddings(model="text-embedding-3-small")`.
  - Mantener la API de `EmbeddingManager` (`embed_documents`, `embed_query`, `embed_text`) para no romper flujos de ingesta/retrieval.
  - Actualizar `settings.embedding_model` a un valor con prefijo `openai:` (p. ej., `openai:text-embedding-3-small`) para seleccionar el proveedor sin tocar m√°s c√≥digo.
  - Ajustar dimensiones por defecto para fallbacks a `settings.default_embedding_dimension = 1536` (coincide con `text-embedding-3-small`).

- Nota: Gestionar `OPENAI_API_KEY` a trav√©s de variables de entorno.
  - Validar en el arranque que `settings.openai_api_key` est√° presente cuando `settings.model_type == "OPENAI"` (ya existe validaci√≥n).
  - Documentar el uso de `EMBEDDING_MODEL=openai:text-embedding-3-small` en `.env`/Render.

[ ] Tarea 2: An√°lisis y Limpieza de Dependencias

- Archivo a modificar: `backend/requirements.txt`

- Acci√≥n: Analizar librer√≠as usadas √∫nicamente por el modelo de embeddings local y eliminar.
  - Candidatas a eliminar y justificaci√≥n:
    - `sentence-transformers`: solo se usa para `SentenceTransformer` en `EmbeddingManager`. Quitarla elimina su cadena de dependencias pesadas.
    - `langchain-huggingface`: importada pero no instanciada/usable en el flujo actual; mantenerla arrastra `transformers` y `torch` indirectamente. Quitarla reduce tama√±o.
    - Indirectas removidas al quitar las anteriores: `transformers`, `torch`, `scipy` (t√≠picamente arrastradas por `sentence-transformers`/`huggingface`).

- Acci√≥n: Asegurar dependencias necesarias para OpenAI.
  - Mantener `langchain-openai`.
  - A√±adir expl√≠citamente `openai>=1.x` (cliente oficial) para asegurar compatibilidad en runtime con `langchain-openai`.
  - Mantener `tiktoken`.

- Acci√≥n: Revisar referencias de logs a librer√≠as locales.
  - Opcional: eliminar/ajustar en `api/app.py` los `logging.getLogger("sentence_transformers"|"transformers")` si ya no est√°n presentes para evitar ruido innecesario.

[ ] Tarea 3: Configuraci√≥n de Entorno

- Acci√≥n: A√±adir `OPENAI_API_KEY` a las variables de entorno en Render y local (`.env`).
  - `OPENAI_API_KEY=<tu_api_key>`
  - `EMBEDDING_MODEL=openai:text-embedding-3-small`
  - `DEFAULT_EMBEDDING_DIMENSION=1536`
  - Mantener el resto de configuraci√≥n de RAG (directorios, batch size, etc.).

[ ] Tarea 4: Verificaci√≥n y Pruebas Locales

- Acci√≥n: Ejecutar el backend localmente despu√©s de los cambios.
  - Comandos sugeridos:
    - Activar entorno y deps: `cd backend && pip install -r requirements.txt`
    - Lanzar API: `python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000`

- Acci√≥n: Prueba End-to-End para confirmar funcionalidad principal.
  - Probar endpoint de ‚Äúsubir documento‚Äù (nueva indexaci√≥n con OpenAI):
    - `POST /api/v1/pdfs/upload` con `multipart/form-data` (`file`=PDF), espera `200` y mensaje de procesamiento en segundo plano.
    - Confirmar listados: `GET /api/v1/pdfs/list`.
  - Probar endpoint de ‚Äúchat‚Äù (b√∫squeda por similitud + respuesta):
    - `POST /api/v1/chat/stream_log` con JSON `{ "input": "Pregunta basada en el PDF" }`.
    - Verificar que responde y que usa contexto del documento (retrieval correcto).
  - Validar m√©tricas b√°sicas:
    - Arranque sin descargar modelos locales.
    - Sin picos de RAM inicial por `torch/transformers`.

4. Beneficios y Resultados Esperados

- Reducci√≥n de Tama√±o:
  - Quitar `sentence-transformers` y `langchain-huggingface` elimina cadenas de dependencias muy pesadas (`torch`, `transformers`, `scipy`).
  - Reducci√≥n estimada de la build: cientos de MB (en muchos entornos, entre ~500 MB y >1 GB), lo cual es cr√≠tico para l√≠mites de Render Free.

- Consumo de RAM:
  - Elimina la carga de modelos locales en el arranque; la RAM inicial baja dr√°sticamente (ahorro t√≠pico de cientos de MB).
  - El uso de `OpenAIEmbeddings` es remoto y ligero; mantiene la memoria estable.

- Velocidad de Deploy:
  - Menos paquetes para descargar/compilar -> deploys mucho m√°s r√°pidos y menos fallos por timeouts.

- Mantenibilidad:
  - Simplifica el c√≥digo: se conserva una √∫nica ruta de embeddings (OpenAI) manteniendo la misma interfaz (`EmbeddingManager`).
  - Se reduce la complejidad de carga perezosa y handling de fallback de modelos locales.

Notas finales:
- La migraci√≥n propuesta no cambia el flujo RAG: la ingesta sigue generando embeddings y almacenando en Chroma; el retrieval contin√∫a calculando el embedding de consulta y usando b√∫squeda por similitud + MMR y reranking sem√°ntico.
- Alinear la dimensi√≥n por defecto con el modelo (`1536` para `text-embedding-3-small`) evita inconsistencias en fallbacks y comparaciones.

5. Impacto en C√≥digo y Archivos (Reducci√≥n estimada)

- `backend/rag/embeddings/embedding_manager.py`
  - Supresi√≥n de la rama local `SentenceTransformer` (carga perezosa y uso): ‚âà 30‚Äì40 l√≠neas menos.
    - `_load_st` y variable `_ST`: ‚âà 8 l√≠neas.
    - Rama `SentenceTransformer` en `__init__`: ‚âà 3‚Äì4 l√≠neas.
    - Rama local en `embed_documents(...)`: ‚âà 10‚Äì12 l√≠neas.
    - Rama local en `embed_query(...)`: ‚âà 8 l√≠neas.
    - Parte de `get_embedding_model()` que carga ST: ‚âà 3‚Äì4 l√≠neas.
- `backend/api/app.py`
  - Remover ajustes de logging espec√≠ficos de `sentence_transformers`/`transformers`: 2‚Äì3 l√≠neas.
- `backend/rag/retrieval/retriever.py`
  - Eliminar import no utilizado `from langchain_huggingface import HuggingFaceEmbeddings`: 1 l√≠nea.
- `backend/requirements.txt`
  - Eliminar: `sentence-transformers`, `langchain-huggingface` (2 l√≠neas menos).
  - A√±adir: `openai>=1.x` (1 l√≠nea m√°s). Neto: ‚àí1 l√≠nea.
- Archivos eliminados: ninguno (se mantiene la arquitectura, solo se simplifica la ruta de embeddings).

Total estimado de reducci√≥n en c√≥digo: ‚âà 35‚Äì50 l√≠neas.
<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>RAG Technical Review &mdash; Revisi&oacute;n T&eacute;cnica Profunda</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="rag-technical-review--revisi√≥n-t√©cnica-profunda">RAG Technical Review ‚Äî Revisi√≥n T√©cnica Profunda</h1>
<p><strong>Tipo de sistema</strong>: RAG experimental/real (no toy, no enterprise cr√≠tico)<br>
<strong>Fecha de an√°lisis</strong>: 2026-01-12<br>
<strong>Objetivo</strong>: Identificar riesgos latentes, bombas de tiempo, y mejoras de alto impacto</p>
<h2 id="-executive-summary">üìã Executive Summary</h2>
<p>Este RAG es t√©cnicamente s√≥lido con patrones bien pensados (embeddings cacheados, MMR, RAG gating con centroide, deduplicaci√≥n por hash, etc.). Sin embargo, hay 7 bombas de tiempo cr√≠ticas que pueden explotar con escala o uso prolongado, y mejoras de alto impacto que son relativamente simples de implementar.</p>
<h3 id="lo-que-est√°-bien-dise√±ado">Lo que est√° bien dise√±ado</h3>
<ul>
<li>‚úÖ Deduplicaci√≥n por <code>content_hash</code> (normalizado) evita duplicados sem√°nticos</li>
<li>‚úÖ Cache en m√∫ltiples niveles (embeddings, RAG results, LLM responses)</li>
<li>‚úÖ RAG gating con centroide (evita retrieval innecesario)</li>
<li>‚úÖ Streaming SSE con timeout en primer chunk</li>
<li>‚úÖ Soporte de MMR para diversidad</li>
</ul>
<hr>
<h2 id="-hallazgos-cr√≠ticos">üö® Hallazgos Cr√≠ticos</h2>
<h3 id="hallazgo-1-chunking-fijo-sin-adaptaci√≥n-a-estructura-del-documento">[HALLAZGO #1] Chunking fijo sin adaptaci√≥n a estructura del documento</h3>
<p><strong>Descripci√≥n</strong>: El sistema usa <code>chunk_size=500</code> y <code>chunk_overlap=50</code> fijos para todos los PDFs (<code>config.py:L102-103</code>). PyMuPDFLoader extrae texto plano y luego se segmenta con ventana deslizante (<code>ingestor.py:L101-104</code>). No hay reconocimiento de:</p>
<ul>
<li>Headers/t√≠tulos</li>
<li>Listas</li>
<li>Tablas</li>
<li>P√°rrafos sem√°nticos</li>
<li>Cambios de secci√≥n</li>
</ul>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>: Con PDFs complejos (t√©cnicos, legales, manuales con tablas), los chunks cortan arbitrariamente en mitad de:</p>
<ul>
<li><strong>Tablas</strong> ‚Üí metadata corrupta en retrieval</li>
<li><strong>Listas numeradas</strong> ‚Üí p√©rdida de contexto secuencial</li>
<li><strong>Headers</strong> que dan contexto cr√≠tico a los p√°rrafos siguientes</li>
</ul>
<p><strong>Escenario real</strong>: un PDF de pricing con tablas ‚Üí el retrieval devuelve &quot;fragmento 3: $450&quot; sin el contexto de qu√© producto es.</p>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Alto</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Alta - Los pdfs/ en utils contienen 20 PDFs reales. Sin inspeccionar su estructura interna, es estad√≠sticamente probable que al menos algunos tengan tablas o listas que se rompan mal.</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - Patr√≥n com√∫n en RAGs que usan chunking fijo. Verificable inspeccionando chunks indexados en Qdrant con retrieve-debug.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>: Implementar chunking sem√°ntico/estructural:</p>
<ol>
<li><strong>Fase 1</strong> (low-hanging fruit): Usar RecursiveCharacterTextSplitter con separadores de p√°rrafos/l√≠neas en vez de ventana deslizante ciega</li>
<li><strong>Fase 2</strong> (ideal): Detectar estructura (ej: pymupdf con layout mode o unstructured library) y chunkar respetando boundaries naturales</li>
<li><strong>Medio camino</strong>: Ajustar <code>min_chunk_length=100</code> m√°s agresivamente y validar que chunks tengan frases completas (no corten mid-sentence)</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>No implementar chunking recursivo ultra-sofisticado (overkill). El 80% del problema se resuelve con separadores inteligentes y validaci√≥n de frases completas.</p>
</div>
<hr>
<h3 id="hallazgo-2-rag-gating-con-centroide-puede-quedar-stale-silenciosamente">[HALLAZGO #2] RAG Gating con centroide puede quedar stale silenciosamente</h3>
<p><strong>Descripci√≥n</strong>: El gating usa un centroide calculado de todos los embeddings en Qdrant (<code>retriever.py:L680-788</code>). El rec√°lculo tiene protecciones:</p>
<ul>
<li>Cache TTL impl√≠cito v√≠a <code>_last_corpus_size_check_time</code></li>
<li>Invalidaci√≥n autom√°tica si cambia <code>count()</code> total</li>
<li>Lock async para evitar recalcs simult√°neos</li>
</ul>
<p><strong>Sin embargo</strong>:</p>
<ul>
<li>Si el cache de Redis se limpia externamente (flush, restart, eviction por memoria), el centroide desaparece</li>
<li>El c√≥digo intenta recalcular en background (<code>L278-286</code>), PERO si no hay event loop corriendo (ej: durante <code>config.py</code> import), el schedule falla silenciosamente</li>
<li>Mientras tanto, <code>gating_async()</code> compara contra <code>self._centroid_embedding = None</code> y hace fail-open (<code>L907</code>): <code>return (&quot;no_centroid&quot;, True)</code> ‚Üí siempre usa RAG sin validaci√≥n sem√°ntica</li>
</ul>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>:</p>
<ul>
<li><strong>Costo oculto</strong>: Si el centroide est√° stale, TODAS las queries pasan gating ‚Üí retrieval masivo + embeddings + LLM context window inflado ‚Üí facturas de OpenAI se disparan</li>
<li><strong>Latencia</strong>: Retrieval innecesario en queries simples (&quot;hola&quot;, &quot;gracias&quot;) que deber√≠an skipear RAG</li>
<li><strong>Calidad degradada</strong>: Context pollution con docs irrelevantes (porque el filtro sem√°ntico est√° roto)</li>
</ul>
<p><strong>Escenario real</strong>:</p>
<ol>
<li>Redis se reinicia por deploy</li>
<li>Durante 5-10 min hasta que el background task recalcula, todas las conversaciones activas hacen retrieval indiscriminado</li>
<li>Admin no se da cuenta hasta ver logs de costos</li>
</ol>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Alto</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Media - Depende de frecuencia de Redis restarts, volumen de tr√°fico concurrente, y si hay monitoreo de costos</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - Code path verificable: ver <code>gating_async()</code> l√≠nea 857-858 donde se agenda sin esperar, y l√≠nea 907 del fallback.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>:</p>
<ol>
<li>
<p><strong>Health check cr√≠tico</strong>: Agregar endpoint <code>/api/v1/rag/gating-health</code> que exponga:</p>
<ul>
<li><code>centroid_loaded: bool</code></li>
<li><code>last_recalc_timestamp</code></li>
<li><code>corpus_size</code></li>
<li>Este endpoint deber√≠a alertar si <code>centroid_loaded == false</code> por &gt;2 minutos</li>
</ul>
</li>
<li>
<p><strong>Persistencia dual</strong>: Adem√°s de Redis, guardar centroide en MongoDB como fallback (peque√±o doc ~1KB). Si cache miss, cargar desde Mongo antes de recalcular.</p>
</li>
<li>
<p><strong>Fail-closed conservador</strong>: En lugar de fail-open, si no hay centroide Y corpus_size &gt; 50, hacer fail-closed (skip RAG) excepto en queries con interrogativos obvios. Evita context pollution masivo.</p>
</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>El c√≥digo ya tiene l√≥gica de invalidaci√≥n robusta (L842-848 detecta cambios de corpus). El problema es la persistencia del centroide fuera de Redis.</p>
</div>
<hr>
<h3 id="hallazgo-3-embeddings-cacheados-sin-version-tracking--incompatibilidad-silenciosa">[HALLAZGO #3] Embeddings cacheados sin version tracking ‚Üí incompatibilidad silenciosa</h3>
<p><strong>Descripci√≥n</strong>: Los embeddings se cachean con key: <code>emb:doc:{model_name}:{sha256(text)}</code> (<code>embedding_manager.py:L80</code>). El <code>model_name</code> viene de <code>settings.embedding_model</code> (ej: &quot;openai:text-embedding-3-small&quot;).</p>
<p><strong>Problema</strong>: Si cambias de modelo (ej: text-embedding-3-small ‚Üí text-embedding-3-large para mejor calidad), las keys cambian... PERO:</p>
<ul>
<li><strong>Qdrant conserva vectores viejos</strong>: Los puntos en Qdrant siguen teniendo embeddings del modelo antiguo</li>
<li><strong>Queries usan modelo nuevo</strong>: El <code>embed_query()</code> genera con el nuevo modelo</li>
<li><strong>Similitud sem√°ntica corrupta</strong>: Est√°s comparando embeddings de dimensiones diferentes o espacios sem√°nticos incompatibles</li>
</ul>
<p><strong>Escenario dram√°tico</strong>:</p>
<ol>
<li>Cambias dimensi√≥n de 1536 ‚Üí 3072</li>
<li>C√≥digo hace fallback a vector cero (<code>L123</code>) si detecta inconsistencia</li>
<li>TODOS los docs retrieval tienen score ~0 ‚Üí RAG &quot;se apaga&quot; silenciosamente</li>
</ol>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>: T√≠pico en evoluci√≥n de sistemas:</p>
<ol>
<li>Pruebas con modelo barato</li>
<li>Upgrade a modelo premium</li>
<li>Olvidas reindexar PDFs</li>
<li>Users reportan &quot;el chatbot ya no responde con contexto&quot;</li>
</ol>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Medio</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Media - Depende de policy de actualizaciones. Si es experimental y est√°n probando modelos, alta probabilidad.</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - El c√≥digo tiene validaci√≥n de dimensi√≥n (<code>L85-88</code>), pero no version tracking.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>:</p>
<ol>
<li>
<p><strong>Metadata en Qdrant</strong>: Agregar <code>embedding_model_version</code> y <code>embedding_dimension</code> a cada punto. Durante retrieval, filtrar solo puntos con modelo compatible.</p>
</li>
<li>
<p><strong>Migration endpoint</strong>: Crear <code>POST /api/v1/rag/migrate-embeddings</code> que:</p>
<ul>
<li>Scroll todos los puntos</li>
<li>Re-embediza con modelo nuevo</li>
<li>Actualiza en batch</li>
<li>Progreso trackeable</li>
</ul>
</li>
<li>
<p><strong>Config validation</strong>: Al arrancar, verificar que <code>settings.default_embedding_dimension</code> coincide con una sample de Qdrant. Si no, loguear WARNING masivo.</p>
</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>El fallback a vector cero es defensivo pero oculta el problema. Mejor: fallar ruidosamente con RuntimeError al detectar incompatibilidad, forzando al admin a tomar acci√≥n.</p>
</div>
<hr>
<h3 id="hallazgo-4-cache-manager-falla-ruidosamente-en-init-si-redis-no-conecta--resuelto">[HALLAZGO #4] Cache manager falla ruidosamente en init si Redis no conecta ‚úÖ RESUELTO</h3>
<p><strong>Descripci√≥n</strong>: <code>cache/manager.py:L25-53</code> inicializa Redis y hace <code>client.ping()</code>. Si falla, lanza RuntimeError y muere el proceso.</p>
<pre><code class="language-python"><span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
    _logger.critical(<span class="hljs-string">f&quot;FALLO CR√çTICO DE REDIS: <span class="hljs-subst">{e}</span>&quot;</span>)
    <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&quot;Conexi√≥n a Redis fallida - Backend detenido&quot;</span>) <span class="hljs-keyword">from</span> e
</code></pre>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>: Entornos t√≠picos:</p>
<ul>
<li><strong>Local dev</strong>: Redis no est√° corriendo ‚Üí uvicorn main:app crashea inmediatamente, confundiendo a desarrolladores nuevos</li>
<li><strong>Staging/Prod</strong>: Redis temporal unavailable (network blip, container restart) ‚Üí todo el backend se cae en vez de degradar gracefully</li>
<li><strong>Docker Compose</strong>: Race condition si el backend arranca antes que Redis ‚Üí restart loop infinito</li>
</ul>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Alto (disponibilidad)</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Alta - README menciona Docker Compose, es com√∫n tener race conditions en startup.</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - C√≥digo expl√≠cito: raise RuntimeError.</td>
</tr>
<tr>
<td><strong>Estado</strong></td>
<td>‚úÖ <strong>RESUELTO</strong> - Implementada degradaci√≥n elegante con retry logic y fallback a InMemoryCache</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong> (IMPLEMENTADO):</p>
<ol>
<li>
<p><strong>Graceful degradation</strong>: Si Redis falla, crear InMemoryCache temporal con WARNING logs masivos. Sistema corre degradado pero funcional. ‚úÖ</p>
</li>
<li>
<p><strong>Retry logic</strong>: En <code>_init_backend()</code>, hacer 3 intentos con exponential backoff antes de fallar. √ötil para race conditions de startup. ‚úÖ</p>
</li>
<li>
<p><strong>Health check separado</strong>: No validar Redis en import-time. Mover la conexi√≥n a un lifespan event de FastAPI y marcar /health como unhealthy si falla. ‚úÖ</p>
</li>
</ol>
<hr>
<h3 id="hallazgo-5-memory-window-size-fijo-sin-paginaci√≥n--explosi√≥n-de-tokens-en-conversaciones-largas">[HALLAZGO #5] Memory window size fijo sin paginaci√≥n ‚Üí explosi√≥n de tokens en conversaciones largas</h3>
<p><strong>Descripci√≥n</strong>: <code>memory/base_memory.py:L115</code> usa <code>.limit(self.window_size)</code> fijo (default=5 mensajes). El historial se inserta en el prompt como string:</p>
<pre><code class="language-python">formatted_hist = self.bot._format_history(hist)
<span class="hljs-comment"># ‚Üí &quot;User: ...\nAssistant: ...\n&quot; √ó N</span>
</code></pre>
<p><strong>Problemas</strong>:</p>
<ul>
<li><strong>Window size en mensajes, no tokens</strong>: 5 mensajes pueden ser 100 tokens o 5000 tokens dependiendo de la verbosidad</li>
<li><strong>Sin truncamiento</strong>: Si un mensaje user/assistant tiene 2000 tokens, entra completo</li>
<li><strong>Prompt explosion</strong>: Con window=5 y mensajes largos, el historial puede consumir 80% del context window del LLM</li>
</ul>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>:</p>
<ul>
<li><strong>Conversaciones t√©cnicas</strong>: User pega c√≥digo o logs largos ‚Üí mensajes de 1000+ tokens</li>
<li><strong>Costo silencioso</strong>: Cada turno procesa tokens masivos de historial innecesario</li>
<li><strong>Latencia</strong>: LLMs lentos con context window grande</li>
</ul>
<p><strong>Escenario</strong>:</p>
<ol>
<li>Usuario hace pregunta t√©cnica con traceback de 50 l√≠neas</li>
<li>Window conserva ese mensaje completo √ó 5 turnos</li>
<li>Cada respuesta paga 5√ó lo esperado en input tokens</li>
</ol>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Medio</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Media - Depende del domain. Si es soporte t√©cnico o compliance (donde users copian docs largos), muy alta.</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - Patr√≥n com√∫n en chatbots. El c√≥digo usa tiktoken para contar tokens en debug (<code>chat/manager.py:L22-31</code>) pero NO para limitar historial.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>:</p>
<ol>
<li><strong>Token-based window</strong>: En lugar de <code>.limit(5)</code>, hacer:</li>
</ol>
<pre><code class="language-python">MAX_HISTORY_TOKENS = <span class="hljs-number">1500</span>
cumulative_tokens = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> msg <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(messages):
    tok = count_tokens(msg[<span class="hljs-string">&#x27;content&#x27;</span>])
    <span class="hljs-keyword">if</span> cumulative_tokens + tok &gt; MAX_HISTORY_TOKENS:
        <span class="hljs-keyword">break</span>
    cumulative_tokens += tok
    history.append(msg)
</code></pre>
<ol start="2">
<li>
<p><strong>Summarization light</strong>: Para conversaciones &gt;10 turnos, resumir turnos antiguos con LLM barato (gpt-3.5-turbo) y guardar en metadata especial <code>{role: &quot;system&quot;, content: &quot;Resumen de conversaci√≥n anterior: ...&quot;}</code>.</p>
</li>
<li>
<p><strong>User-facing truncation</strong>: Si un mensaje excede 500 tokens, truncar con &quot;...&quot; y guardar el completo en DB para auditabilidad.</p>
</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>El c√≥digo ya tiene tiktoken importado (lazy load), reusar eso. No agregar dependencias nuevas.</p>
</div>
<hr>
<h3 id="hallazgo-6-sin-rate-limiting-en-ingesta-de-pdfs--dos-por-upload-masivo">[HALLAZGO #6] Sin rate limiting en ingesta de PDFs ‚Üí DoS por upload masivo</h3>
<p><strong>Descripci√≥n</strong>: <code>api/routes/pdf_routes.py</code> (no inspeccionado directamente, inferido de README) permite <code>POST /api/v1/pdfs/upload</code>. No hay evidencia de:</p>
<ul>
<li>Rate limiting en este endpoint espec√≠fico</li>
<li>Queue/throttling de procesamiento</li>
<li>L√≠mite de PDFs concurrentes en ingesta</li>
</ul>
<p>El <code>RAGIngestor.ingest_single_pdf()</code> (<code>ingestor.py:L91-169</code>) es bloqueante y cpu/io intensivo:</p>
<ul>
<li><code>PyMuPDFLoader.load()</code> ‚Üí extracci√≥n de texto (CPU)</li>
<li><code>embedding_manager.embed_documents()</code> ‚Üí API calls masivos a OpenAI</li>
<li><code>vector_store.add_documents()</code> en batches ‚Üí upserts a Qdrant</li>
</ul>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>: Escenario de abuso (malicious o no):</p>
<ol>
<li>Admin sube 10 PDFs de 200 p√°ginas cada uno simult√°neamente</li>
<li>Backend procesa 10 PDFs en paralelo ‚Üí 2000 p√°ginas √ó chunks = ~8000 embed calls a OpenAI</li>
<li>OpenAI rate limits ‚Üí errores en cascada</li>
<li>Qdrant se satura con upserts concurrentes</li>
<li>Otros requests (chat) se bloquean porque FastAPI thread pool est√° consumido</li>
</ol>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Alto</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Media - Si solo hay 1-2 admins internos, baja. Si hay UI de PDF upload expuesta, media-alta.</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Media - No vi el c√≥digo de rutas directamente, pero <code>RAGIngestor</code> evidentemente no tiene throttling interno.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>:</p>
<ol>
<li>
<p><strong>Queue ingesta</strong>: Usar <code>asyncio.Queue</code> con workers limitados (ej: 2 workers, 1 PDF por worker). Uploads van a cola, se procesan secuencialmente.</p>
</li>
<li>
<p><strong>Rate limit endpoint</strong>: Si slowapi ya est√° en dependencies (<code>requirements.txt:L48</code>), aplicar decorador <code>@limiter.limit(&quot;5/hour&quot;)</code> en upload.</p>
</li>
<li>
<p><strong>Progress tracking</strong>: Guardar ingesta en colecci√≥n <code>pdf_ingestion_jobs</code> con estados <code>queued/processing/completed/failed</code>. UI polling de estado.</p>
</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>El sistema ya tiene <code>batch_size=100</code> configurado (<code>config.py:L115</code>), pero eso es por embedding batch, no l√≠mite de concurrencia global.</p>
</div>
<hr>
<h3 id="hallazgo-7-version-pins-demasiado-estrechos-en-dependencies--lock-in-riesgoso">[HALLAZGO #7] Version pins demasiado estrechos en dependencies ‚Üí lock-in riesgoso</h3>
<p><strong>Descripci√≥n</strong>: <code>requirements.txt</code> tiene versiones:</p>
<pre><code>langchain-core==0.1.52  # EXACT pin
langchain==0.1.17
langchain-community==0.0.36
langchain-openai==0.0.5
</code></pre>
<p>Exact pins (<code>==</code>) en lugar de rangos compatibles (<code>&gt;=X,&lt;Y</code>).</p>
<p><strong>Problemas</strong>:</p>
<ul>
<li><strong>Security patches bloqueados</strong>: Si LangChain publica 0.1.53 con CVE fix, no se actualiza autom√°ticamente</li>
<li><strong>Dependency hell</strong>: Si otra lib requiere <code>langchain-core&gt;=0.1.53</code>, conflicto irreconciliable</li>
<li><strong>Obsolescencia</strong>: LangChain es un proyecto en r√°pida evoluci√≥n. Versiones de enero 2024 (0.0.36 community) pueden tener bugs ya resueltos</li>
</ul>
<p><strong>Por qu√© puede ser una bomba de tiempo</strong>: En 6-12 meses:</p>
<ol>
<li>Quieres usar feature nuevo de LangChain (ej: mejor streaming)</li>
<li>Upgrade requiere refactor porque la API cambi√≥</li>
<li>Acumulas deuda t√©cnica</li>
</ol>
<table>
<thead>
<tr>
<th>M√©trica</th>
<th>Valor</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Nivel de severidad</strong></td>
<td>Bajo</td>
</tr>
<tr>
<td><strong>Probabilidad de que aplique a ESTE RAG</strong></td>
<td>Alta (inevitable con el tiempo)</td>
</tr>
<tr>
<td><strong>Nivel de confianza en el diagn√≥stico</strong></td>
<td>Alta - Archivos de requirements est√°ndar.</td>
</tr>
</tbody>
</table>
<p><strong>Recomendaci√≥n conceptual</strong>:</p>
<ol>
<li><strong>Semver ranges</strong>: Cambiar a:</li>
</ol>
<pre><code>langchain-core&gt;=0.1.52,&lt;0.3.0
langchain&gt;=0.1.17,&lt;0.3.0
</code></pre>
<p>Permite patches/minor updates, bloquea breaking changes.</p>
<ol start="2">
<li>
<p><strong>Dependabot</strong>: Si usas GitHub, habilitar Dependabot para PRs autom√°ticos de seguridad.</p>
</li>
<li>
<p><strong>Test suite robusto</strong>: Antes de flexibilizar pins, asegurar que tienes tests que detecten breaking changes en upgrades.</p>
</li>
</ol>
<div class="markdown-alert markdown-alert-note"><p class="markdown-alert-title"><svg class="octicon octicon-info mr-2" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13ZM6.5 7.75A.75.75 0 0 1 7.25 7h1a.75.75 0 0 1 .75.75v2.75h.25a.75.75 0 0 1 0 1.5h-2a.75.75 0 0 1 0-1.5h.25v-2h-.25a.75.75 0 0 1-.75-.75ZM8 6a1 1 0 1 1 0-2 1 1 0 0 1 0 2Z"></path></svg>Note</p><p>Este es el trade-off cl√°sico: stability vs freshness. Para RAG experimental, inclinarse hacia freshness (rangos amplios). Para production cr√≠tico, lockfiles estrictos.</p>
</div>
<hr>
<h2 id="-hallazgos-menores">üü° Hallazgos Menores</h2>
<h3 id="hallazgo-8-similarity-threshold-muy-bajo-03--ruido-en-retrieval">[HALLAZGO #8] Similarity threshold muy bajo (0.3) ‚Üí ruido en retrieval</h3>
<p><strong>Descripci√≥n</strong>: <code>config.py:L111</code>: <code>similarity_threshold: float = 0.3</code>. Con similitud coseno normalizada (0-1), 0.3 es MUY permisivo.</p>
<p><strong>Impacto</strong>:</p>
<ul>
<li>Documentos marginalmente relevantes pasan el filtro</li>
<li>Context polluted con info tangencial</li>
<li>LLM puede generar respuestas inconsistentes</li>
</ul>
<p><strong>Recomendaci√≥n</strong>: Experimentar con threshold 0.5-0.6. Para queries cr√≠ticas, ser m√°s conservador (devolver menos docs pero m√°s relevantes).</p>
<p><strong>Severidad</strong>: Bajo | <strong>Probabilidad</strong>: Media | <strong>Confianza</strong>: Alta</p>
<hr>
<h3 id="hallazgo-9-mock-mode-en-producci√≥n-es-un-riesgo-de-seguridad">[HALLAZGO #9] Mock mode en producci√≥n es un riesgo de seguridad</h3>
<p><strong>Descripci√≥n</strong>: <code>config.py:L58</code>: <code>mock_mode: bool = Field(default=False, env=&quot;MOCK_MODE&quot;)</code>. Si alguien accidentalmente setea <code>MOCK_MODE=true</code> en producci√≥n:</p>
<ul>
<li>Todos los embeddings son vectores cero (<code>embedding_manager.py:L58-63</code>)</li>
<li>LLM responde con texto mock (<code>bot.py:L250-252</code>)</li>
<li>Users reciben respuestas fake sin saber</li>
</ul>
<p><strong>Recomendaci√≥n</strong>: Validaci√≥n en <code>config.py</code>:</p>
<pre><code class="language-python"><span class="hljs-keyword">if</span> self.environment == <span class="hljs-string">&quot;production&quot;</span> <span class="hljs-keyword">and</span> self.mock_mode:
    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;MOCK_MODE no permitido en producci√≥n&quot;</span>)
</code></pre>
<p><strong>Severidad</strong>: Medio | <strong>Probabilidad</strong>: Baja | <strong>Confianza</strong>: Alta</p>
<hr>
<h3 id="hallazgo-10-jwt-secret-validation-solo-en-production--devstaging-vulnerables">[HALLAZGO #10] JWT secret validation solo en production ‚Üí dev/staging vulnerables</h3>
<p><strong>Descripci√≥n</strong>: <code>config.py:L278-289</code> valida <code>JWT_SECRET</code> solo si <code>environment == &quot;production&quot;</code>. En staging/dev, puede estar ausente o weak ‚Üí autenticaci√≥n comprometida.</p>
<p><strong>Recomendaci√≥n</strong>: Validar <code>JWT_SECRET</code> en TODOS los environments, con mensaje claro de que puede ser dummy en dev local.</p>
<p><strong>Severidad</strong>: Medio | <strong>Probabilidad</strong>: Media si staging es p√∫blico | <strong>Confianza</strong>: Alta</p>
<hr>
<h2 id="-cosas-bien-dise√±adas-reconocimiento">‚úÖ Cosas Bien Dise√±adas (Reconocimiento)</h2>
<ol>
<li>
<p><strong>Deduplicaci√≥n robusta</strong> (<code>ingestor.py:L51-62, L115-118</code>): <code>content_hash_global</code> normalizado evita duplicados sem√°nticos. Excelente.</p>
</li>
<li>
<p><strong>Cache invalidation inteligente</strong> (<code>retriever.py:L999-1005</code>): Usar prefijos (<code>rag:</code>, <code>vs:</code>, <code>resp:</code>) permite invalidaci√≥n quir√∫rgica. Bien pensado.</p>
</li>
<li>
<p><strong>Streaming con timeout</strong> (<code>chat/manager.py:L323</code>): Esperar primer chunk con timeout evita hang infinito. UX-forward.</p>
</li>
<li>
<p><strong>Centroid lock async</strong> (<code>retriever.py:L224-228, L687-688</code>): Evita race conditions en rec√°lculo. Sofisticado.</p>
</li>
<li>
<p><strong>Payload indexes en Qdrant</strong> (<code>vector_store.py:L107-132</code>): Crear √≠ndices en <code>source</code>, <code>pdf_hash</code>, <code>content_hash_global</code> acelera filtrado. Performance-conscious.</p>
</li>
</ol>
<hr>
<h2 id="-priorizaci√≥n-de-fixes">üìä Priorizaci√≥n de Fixes</h2>
<table>
<thead>
<tr>
<th>Hallazgo</th>
<th>Severidad</th>
<th>Impacto</th>
<th>Esfuerzo Fix</th>
<th>Prioridad</th>
</tr>
</thead>
<tbody>
<tr>
<td>#4 Cache manager crash ‚úÖ</td>
<td>Alto</td>
<td>Alto</td>
<td>Bajo (2h)</td>
<td>üî¥ P0 ‚úÖ RESUELTO</td>
</tr>
<tr>
<td>#1 Chunking fijo</td>
<td>Alto</td>
<td>Alto</td>
<td>Medio (1 d√≠a)</td>
<td>üî¥ P0</td>
</tr>
<tr>
<td>#2 Centroid stale</td>
<td>Alto</td>
<td>Medio</td>
<td>Medio (4h)</td>
<td>üü† P1</td>
</tr>
<tr>
<td>#6 PDF upload DoS</td>
<td>Alto</td>
<td>Medio</td>
<td>Medio (4h)</td>
<td>üü† P1</td>
</tr>
<tr>
<td>#3 Embedding version</td>
<td>Medio</td>
<td>Alto</td>
<td>Alto (2 d√≠as)</td>
<td>üü† P1</td>
</tr>
<tr>
<td>#5 Token explosion</td>
<td>Medio</td>
<td>Medio</td>
<td>Medio (4h)</td>
<td>üü° P2</td>
</tr>
<tr>
<td>#9 Mock mode</td>
<td>Medio</td>
<td>Bajo</td>
<td>Bajo (15min)</td>
<td>üü° P2</td>
</tr>
<tr>
<td>#7 Version pins</td>
<td>Bajo</td>
<td>Bajo</td>
<td>Bajo (30min)</td>
<td>üü¢ P3</td>
</tr>
<tr>
<td>#8 Similarity threshold</td>
<td>Bajo</td>
<td>Bajo</td>
<td>Bajo (test)</td>
<td>üü¢ P3</td>
</tr>
<tr>
<td>#10 JWT staging</td>
<td>Medio</td>
<td>Bajo</td>
<td>Bajo (15min)</td>
<td>üü¢ P3</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="-quick-wins-bajo-esfuerzo-alto-impacto">üéØ Quick Wins (Bajo esfuerzo, alto impacto)</h2>
<ul>
<li>‚úÖ <strong>Fix #4</strong> (Cache graceful degradation): 2 horas ‚Üí evitas downtime total - <strong>COMPLETADO</strong></li>
<li><strong>Fix #9</strong> (Mock mode validation): 15 min ‚Üí evitas incidente catastr√≥fico</li>
<li><strong>Fix #10</strong> (JWT validation): 15 min ‚Üí hardening b√°sico</li>
<li><strong>Ajustar similarity_threshold a 0.5</strong>: 0 c√≥digo, solo config</li>
</ul>
<hr>
<h2 id="-tests-de-validaci√≥n-recomendados">üî¨ Tests de Validaci√≥n Recomendados</h2>
<p>Para verificar estos issues en tu sistema actual:</p>
<h3 id="test-1-centroid-staleness">Test 1: Centroid Staleness</h3>
<pre><code class="language-bash"><span class="hljs-comment"># Limpia Redis</span>
redis-cli FLUSHALL

<span class="hljs-comment"># Hacer query inmediatamente</span>
curl -X POST http://localhost:8000/api/v1/chat \
  -d <span class="hljs-string">&#x27;{&quot;input&quot;:&quot;¬øCu√°l es el precio?&quot;}&#x27;</span> \
  -H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span>

<span class="hljs-comment"># Revisar logs: deber√≠a ver &quot;no_centroid&quot; en gating_reason</span>
</code></pre>
<h3 id="test-2-chunking-quality">Test 2: Chunking Quality</h3>
<pre><code class="language-bash"><span class="hljs-comment"># Endpoint de debug</span>
curl -X POST http://localhost:8000/api/v1/rag/retrieve-debug \
  -d <span class="hljs-string">&#x27;{&quot;query&quot;:&quot;tabla de precios&quot;, &quot;k&quot;:5}&#x27;</span> \
  -H <span class="hljs-string">&quot;Authorization: Bearer &lt;token&gt;&quot;</span>

<span class="hljs-comment"># Inspeccionar `text` de cada item:</span>
<span class="hljs-comment"># - ¬øSe cort√≥ a mitad de celda?</span>
<span class="hljs-comment"># - ¬øTiene contexto del header de tabla?</span>
</code></pre>
<h3 id="test-3-token-explosion">Test 3: Token Explosion</h3>
<pre><code class="language-python"><span class="hljs-comment"># Script Python</span>
<span class="hljs-keyword">import</span> requests

long_message = <span class="hljs-string">&quot;Mi c√≥digo:\n&quot;</span> + (<span class="hljs-string">&quot;x = 1\n&quot;</span> * <span class="hljs-number">1000</span>)  <span class="hljs-comment"># 1000 l√≠neas</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):
    requests.post(<span class="hljs-string">&quot;http://localhost:8000/api/v1/chat&quot;</span>, json={
        <span class="hljs-string">&quot;input&quot;</span>: long_message,
        <span class="hljs-string">&quot;conversation_id&quot;</span>: <span class="hljs-string">&quot;test_session&quot;</span>,
        <span class="hljs-string">&quot;debug_mode&quot;</span>: <span class="hljs-literal">True</span>
    })
    <span class="hljs-comment"># Revisar debug_info.input_tokens ‚Üí deber√≠a crecer linealmente</span>
</code></pre>
<hr>
<h2 id="-mejoras-arquitecturales-futuro">üí° Mejoras Arquitecturales (Futuro)</h2>
<p>Estas NO son urgentes, pero aumentar√≠an robustez:</p>
<ol>
<li><strong>Observability</strong>: Integrar OpenTelemetry para traces distribuidos (RAG ‚Üí Qdrant ‚Üí OpenAI ‚Üí LLM)</li>
<li><strong>Hybrid search</strong>: Combinar bm25 (keyword) + vector similarity para queries con t√©rminos t√©cnicos espec√≠ficos</li>
<li><strong>Query classification</strong>: Usar LLM barato para clasificar queries en categories (faq, technical, small_talk) y rutear a pipelines optimizados</li>
<li><strong>Async PDF processing</strong>: Mover ingesta a Celery/RQ workers separados del proceso FastAPI</li>
</ol>
<hr>
<h2 id="-conclusi√≥n">üìù Conclusi√≥n</h2>
<p>Este RAG tiene fundamentos s√≥lidos (cache inteligente, gating, deduplicaci√≥n). Los problemas principales son:</p>
<ul>
<li>‚úÖ <strong>Bombas de tiempo operacionales</strong> (cache fail <s>, centroid stale</s>) ‚Üí fix con health checks + persistencia - <strong>Cache manager resuelto</strong></li>
<li><strong>Calidad de retrieval</strong> limitada por chunking naive ‚Üí mejorar con estructura sem√°ntica</li>
<li><strong>Costos ocultos</strong> (token explosion, rate limiting) ‚Üí implementar limits conservadores</li>
</ul>
<p><strong>Siguiente paso inmediato</strong>: Implementar fixes P0 (<s>#4</s> ‚úÖ y #1) en una branch separada y validar en staging antes de production.</p>

            <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
            
        </body>
        </html>